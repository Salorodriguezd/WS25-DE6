# WS25-DE6

# Supply Chain Delivery Delay Prediction from Multi-Source Data

This repository contains the technical implementation for Part II of the Data Engineering project. It implements an endâ€‘toâ€‘end pipeline that integrates two independent supply chain datasets to predict shipment delivery delays, following the approved project proposal.

## 1. Project overview

- **Goal**: Predict whether an order will be delivered late using multiâ€‘source supply chain data.
- **Datasets**:
  - DataCo Supply Chain Dataset (orderâ€‘level information)
  - SCMS / Shipment Delivery History Dataset (routeâ€‘ and shipmentâ€‘level information)
- **Approach**:
  - Clean and harmonize both datasets
  - Merge them into a single analytical table
  - Engineer temporal, route, shipment, product, and customer features
  - Train baseline and boosted models
  - Answer RQ1â€“RQ3 with reproducible figures and tables

About the technical description, please visit our [Wiki](https://github.com/Salorodriguezd/WS25-DE6/wiki) ! ðŸ§

## 2. Research questions

- **RQ1**: How can a machine learning pipeline be designed to remain robust when integrating multiâ€‘source supply chain data?
- **RQ2**: To what extent can independent supply chain datasets be merged to improve delay prediction?
- **RQ3**: What are the most significant predictors of delivery delay?

Each RQ is backed by at least one table and one figure, all generated directly from code.

## 3. Datasets and links

Please download the original datasets from Kaggle before running the pipeline:

- **DataCo Supply Chain Dataset**  
  - Link: [DataCo Supply Chain Dataset:](https://www.kaggle.com/datasets/evilspirit05/datasupplychain)
- **SCMS / Shipment Delivery History Dataset**  
  - Link: [Supply Chain Shipment Pricing Data
](https://www.kaggle.com/datasets/divyeshardeshana/supply-chain-shipment-pricing-data?utm_source=chatgpt.com)

After downloading, place the raw CSVs under `data/raw/` (see below), or adjust the paths in the ingestion/cleaning scripts if needed.

## 4. Repository structure

```text
WS25-DE6/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ project_pipeline_dag.py        # Airflow DAG (pipeline definition)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/               # (optional) raw data download / loading scripts
â”‚   â”œâ”€â”€ data_cleaning/                # cleaning and preprocessing for DataCo / SCMS
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”‚   â”œâ”€â”€ merge_multisource.py      # merge DataCo + SCMS into merged_dataco_scms.csv
â”‚   â”‚   â””â”€â”€ engineer_features.py      # create engineered features on merged dataset
â”‚   â”œâ”€â”€ modeling/
â”‚   â”‚   â”œâ”€â”€ train_models_dataco.py    # RQ1: baseline models, robustness, feature importance
â”‚   â”‚   â”œâ”€â”€ train_models_rq2.py       # RQ2: single vs multiâ€‘source comparisons
â”‚   â”‚   â””â”€â”€ train_models_rq3.py       # RQ3: significant predictors (importance + SHAP)
â”‚   â””â”€â”€ evaluation/                   # (optional) helper scripts for exporting tables, etc.
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # (not tracked) original Kaggle CSVs
â”‚   â”œâ”€â”€ intermediate/                 # (optional) cleaned versions
â”‚   â””â”€â”€ merged/                       # merged and featureâ€‘engineered datasets
â”‚       â”œâ”€â”€ merged_dataco_scms.csv
â”‚       â””â”€â”€ merged_with_engineered_features.csv
â”‚
â”œâ”€â”€ figures/                          # autoâ€‘generated PDF figures
â”‚   â”œâ”€â”€ RQ1_Fig1_feature_importance.pdf
â”‚   â”œâ”€â”€ RQ1_Fig2_time_based_performance.pdf
â”‚   â”œâ”€â”€ RQ2_Fig3_shap_multisource.pdf
â”‚   â”œâ”€â”€ RQ3_Fig4_relative_importance.pdf
â”‚   â”œâ”€â”€ RQ3_Fig5_shap_summary.pdf
â”‚   â””â”€â”€ RQ3_Fig6_delay_vs_leadtime.pdf
â”‚
â”œâ”€â”€ tables/                           # autoâ€‘generated CSV tables
â”‚   â”œâ”€â”€ RQ1_Table1_model_performance.csv
â”‚   â”œâ”€â”€ RQ1_Table2_time_based_performance.csv
â”‚   â”œâ”€â”€ RQ2_Table3_multisource_vs_single.csv
â”‚   â”œâ”€â”€ RQ2_Table4_feature_comparison.csv
â”‚   â”œâ”€â”€ RQ2_Table5_engineered_features.csv
â”‚   â””â”€â”€ RQ3_Table6_top_predictors.csv
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## 5. How to run the pipeline

### 5.1. Environment setup

Create and activate a virtual environment, then install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate        # Windows Git Bash: source .venv/Scripts/activate
pip install -r requirements.txt
```

Make sure the raw Kaggle CSV files are downloaded (see Dataset links in Section 3) and placed under `data/raw/`:

- `data/raw/DataCoSupplyChainDataset.csv`
- `data/raw/SCMS_Delivery_History_Dataset.csv`

If your raw files have different names, adjust the paths in the ingestion scripts (`src/data_ingestion/load_data_*.py`).

***

### 5.2. Data cleaning

Before merging or modeling, each dataset must be cleaned independently.

#### Clean DataCo dataset

```bash
python -m src.data_ingestion.run_DataCo_pipeline
```

This step:

- Loads `data/raw/DataCoSupplyChainDataset.csv`
- Parses dates, removes duplicates, handles missing values
- Saves the cleaned dataset to `data/DataCo_clean_dates_ddmmyyyy.csv`


#### Clean SCMS dataset

```bash
python -m src.data_ingestion.run_DataSCMS_pipeline
```

This step:

- Loads `data/raw/SCMS_Delivery_History_Dataset.csv`
- Parses delivery dates and creates the `Late_delivery_risk` target column based on:
    - `Late_delivery_risk = 1` if `Delivered to Client Date > Scheduled Delivery Date`
    - `Late_delivery_risk = 0` otherwise (on time or early)
- Cleans categorical variables and numeric columns
- Saves the cleaned dataset to `data/SCMS_cleaned.csv`

After this step, both cleaned datasets are ready for integration.

***

### 5.3. Feature engineering

Now that both datasets are cleaned, we can merge them and create engineered features.

#### 1) Merge DataCo + SCMS into a single table

```bash
python -m src.feature_engineering.merge_multisource
```

This step:

- Reads `data/DataCo_clean_dates_ddmmyyyy.csv` and `data/SCMS_cleaned.csv`
- Performs a left join on `Country` (or route-level aggregates)
- Aligns date formats and keys
- Saves the merged multi-source table to:
    - `data/merged/merged_dataco_scms.csv`


#### 2) Create engineered features on the merged table

```bash
python -m src.feature_engineering.engineer_features
```

This step:

- Loads `data/merged/merged_dataco_scms.csv`
- Creates high-level engineered features:
    - `lead_time_days` â€“ time from order to shipping
    - `ship_mode_risk` â€“ historical late-delivery rate per shipping mode
    - `customer_delay_history` â€“ late-delivery rate per customer
    - `product_delay_rate` â€“ delay rate per product category
    - `route_delay_rate` â€“ delay rate per (country, shipping mode) combination
    - `shipment_weight` â€“ SCMS-derived average weight
- Saves the feature-engineered dataset to:
    - `data/merged/merged_with_engineered_features.csv`

All RQ1â€“RQ3 modeling scripts use this file as their main input.

***

### 5.4. Modeling (RQ1, RQ2, RQ3)

With the feature-engineered dataset ready, run each modeling script to generate figures and tables.

#### RQ1 â€“ Robust pipeline and baseline models

```bash
python -m src.modeling.train_models_dataco
```

**Outputs**:

- `tables/RQ1_Table1_model_performance.csv` â€“ Accuracy/Recall/F1 for Logistic Regression, Random Forest, XGBoost
- `tables/RQ1_Table2_time_based_performance.csv` â€“ ROC-AUC across time-based splits
- `figures/RQ1_Fig1_feature_importance.pdf` â€“ Top 10 features from Random Forest
- `figures/RQ1_Fig2_time_based_performance.pdf` â€“ Line plot of ROC-AUC over time


#### RQ2 â€“ Single-source vs multi-source comparison

```bash
python -m src.modeling.train_models_rq2
```

**Outputs**:

- `tables/RQ2_Table3_multisource_vs_single.csv` â€“ Performance comparison (DataCo vs SCMS vs Merged)
- `tables/RQ2_Table4_feature_comparison.csv` â€“ Feature type availability matrix
- `tables/RQ2_Table5_engineered_features.csv` â€“ Catalog of engineered features
- `figures/RQ2_Fig3_shap_multisource.pdf` â€“ SHAP summary plot


#### RQ3 â€“ Significant predictors of delay

```bash
python -m src.modeling.train_models_rq3
```

**Outputs**:

- `tables/RQ3_Table6_top_predictors.csv` â€“ Top predictors ranked by importance
- `figures/RQ3_Fig4_relative_importance.pdf` â€“ Bar chart of predictor importance
- `figures/RQ3_Fig5_shap_summary.pdf` â€“ SHAP beeswarm plot
- `figures/RQ3_Fig6_delay_vs_leadtime.pdf` â€“ Delay probability by lead time bucket

All figures and tables are saved in PDF and CSV formats, ready for submission.

***

### 5.5. Optional: Export tables to Excel

If you need Excel versions of the tables for submission:

```bash
python src/evaluation/export_tables_to_excel.py
```

This converts all CSV tables in `tables/` to `.xlsx` format.

***

### 5.6. Complete pipeline summary

To run the entire pipeline from raw data to final outputs:

```bash
# 1. Clean datasets
python -m src.data_ingestion.run_DataCo_pipeline
python -m src.data_ingestion.run_DataSCMS_pipeline

# 2. Merge and engineer features
python -m src.feature_engineering.merge_multisource
python -m src.feature_engineering.engineer_features

# 3. Generate all RQ outputs
python -m src.modeling.train_models_dataco
python -m src.modeling.train_models_rq2
python -m src.modeling.train_models_rq3

# 4. (Optional) Export to Excel
python src/evaluation/export_tables_to_excel.py
```

All outputs will be saved under `figures/` and `tables/`.

***

## 6. Airflow DAG

The DAG in `dags/project_pipeline_dag.py` mirrors the project pipeline with tasks

The DAG in `dags/project_pipeline_dag.py` mirrors the project pipeline with tasks such as:

- `extract_dataco`, `extract_scms`
- `clean_dataco`, `clean_scms`
- `merge_multisource`
- `engineer_features`
- `train_rq1_models`, `train_rq2_models`, `train_rq3_models`
- `generate_figures_and_tables`

The DAG was developed and tested with:

- **Apache Airflow**: `3.1.5`

To run the DAG locally:

1. Ensure Airflow is installed in your environment.
2. Copy `dags/project_pipeline_dag.py` into your Airflow `dags/` directory (or point `AIRFLOW_HOME/dags` to this repositoryâ€™s `dags/` folder).
3. Initialize Airflow and start the services:

   ```bash
   airflow db init
   airflow webserver
   airflow scheduler
   ```
4. Open the Airflow UI (by default at `http://localhost:8080`), enable the project DAG, and trigger a run.

The DAG will execute the same steps as the manual CLI commands described in Section 5, and will generate all figures and tables under `figures/` and `tables/`.



## 7. Roles and contributors

- Group: **WS25-DE06**

- **Technical Lead: Seoyeon Kim** â€“ pipeline design, modeling code, figures, tables

- **Documentation & Presentation Lead: Salome Rodriguez Donoso** â€“ report writing, slides, alignment between code and narrative

